{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862c917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Config, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26913398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Config, AdamW, get_linear_schedule_with_warmup, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, num_heads, head_dim, rank, alpha):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.lora_A = torch.nn.Parameter(torch.Tensor(num_heads, head_dim, rank))\n",
    "        self.lora_B = torch.nn.Parameter(torch.Tensor(num_heads, rank, head_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.lora_A)\n",
    "        torch.nn.init.xavier_normal_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_adjustment = self.alpha * torch.einsum('hmr,hrn->hmn', self.lora_A, self.lora_B)\n",
    "        return x + lora_adjustment\n",
    "\n",
    "class GPT2AttentionWithLoRA(GPT2Attention):\n",
    "    def __init__(self, config, is_cross_attention=False):\n",
    "        super().__init__(config, is_cross_attention)\n",
    "        self.lora = LoRALayer(config.num_attention_heads, config.hidden_size // config.num_attention_heads, config.lora_rank, config.lora_alpha)\n",
    "\n",
    "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        query = query + self.lora(query)\n",
    "        return super()._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "class GPT2LMHeadModelWithLoRA(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        for i, block in enumerate(self.transformer.h):\n",
    "            block.attn.c_attn = GPT2AttentionWithLoRA(config, is_cross_attention=False).c_attn\n",
    "            block.attn.c_proj = GPT2AttentionWithLoRA(config, is_cross_attention=False).c_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc4dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Config\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.file_path = file_path\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "\n",
    "        # 计算文件中的行数\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            self.num_lines = sum(1 for line in file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_lines\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 读取指定行的数据\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            for _ in range(i):\n",
    "                next(file)\n",
    "            line = next(file).strip()\n",
    "\n",
    "        # 编码并截断\n",
    "        tokens = self.tokenizer.encode(line, add_special_tokens=True)\n",
    "        tokens = tokens[:self.block_size]\n",
    "\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# 加载分词器和配置\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "config.lora_rank = 4\n",
    "config.lora_alpha = 32\n",
    "\n",
    "# 创建数据集\n",
    "file_path = 'D:\\\\wet_files\\\\reduced_train_part_0.txt'\n",
    "block_size = 128\n",
    "dataset = TextDataset(tokenizer, file_path, block_size)\n",
    "\n",
    "# 创建数据加载器\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0861db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.file_path = file_path\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "\n",
    "        # 计算文件中的行数\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            self.num_lines = sum(1 for line in file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_lines\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 读取指定行的数据\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            for _ in range(i):\n",
    "                next(file)\n",
    "            line = next(file).strip()\n",
    "\n",
    "        # 编码并截断\n",
    "        tokens = self.tokenizer.encode(line, add_special_tokens=True)\n",
    "        tokens = tokens[:self.block_size]\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1127642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 对批次中的所有句子进行填充\n",
    "    padded_batch = pad_sequence([torch.tensor(item, dtype=torch.long) for item in batch], batch_first=True, padding_value=0)\n",
    "    return padded_batch\n",
    "\n",
    "# 创建数据加载器\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98709c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_23300\\2104972750.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_batch = pad_sequence([torch.tensor(item, dtype=torch.long) for item in batch], batch_first=True, padding_value=0)\n",
      "C:\\Users\\86135\\AppData\\Local\\Temp\\ipykernel_23300\\2104972750.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_batch = pad_sequence([torch.tensor(item, dtype=torch.long) for item in batch], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average Loss: 4.410923948986102\n",
      "Epoch: 1, Average Loss: 3.6028256418558424\n",
      "Epoch: 2, Average Loss: 3.486058124406189\n"
     ]
    }
   ],
   "source": [
    "# 创建带 LoRA 的 GPT-2 模型\n",
    "model = GPT2LMHeadModelWithLoRA(config)\n",
    "\n",
    "# 设置模型的设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 设置优化器和训练参数\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(data_loader) * 3  # 假设训练 3 个 epoch\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {'input_ids': batch.to(device), 'labels': batch.to(device)}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch: {epoch}, Average Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ca81cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 4239 lines and saved to D:\\wet_files\\reduced_valid_part_0.txt\n"
     ]
    }
   ],
   "source": [
    "def reduce_data_size(input_file_path, output_file_path, target_size_mb=1):\n",
    "    # 计算目标大小（以字节为单位）\n",
    "    target_size_bytes = target_size_mb * 1024 * 1024\n",
    "\n",
    "    # 读取原始数据\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        data = input_file.readlines()\n",
    "\n",
    "    # 计算需要保留的行数\n",
    "    total_bytes = 0\n",
    "    num_lines_to_keep = 0\n",
    "    for line in data:\n",
    "        total_bytes += len(line.encode('utf-8'))\n",
    "        if total_bytes <= target_size_bytes:\n",
    "            num_lines_to_keep += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # 截取数据以匹配目标大小\n",
    "    reduced_data = data[:num_lines_to_keep]\n",
    "\n",
    "    # 保存截取后的数据到新文件\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.writelines(reduced_data)\n",
    "\n",
    "    print(f\"Data has been reduced to {len(reduced_data)} lines and saved to {output_file_path}\")\n",
    "\n",
    "# 调用函数\n",
    "input_file_path = 'D:\\\\wet_files\\\\valid_part_0.txt'\n",
    "output_file_path = 'D:\\\\wet_files\\\\reduced_valid_part_0.txt'\n",
    "reduce_data_size(input_file_path, output_file_path, target_size_mb=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d3641e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to D:\\wet_files\\trained_model.pt\n"
     ]
    }
   ],
   "source": [
    "train_file_path = 'D:\\\\wet_files\\\\reduced_train_part_0.txt'\n",
    "model_save_path = train_file_path.replace('reduced_train_part_0.txt', 'trained_model.pt')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e850b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 2120/2120, Current Batch Loss: 3.5115964412689217\n",
      "Validation Loss: 3.53309985174323\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 定义 TextDataset 类和 collate_fn 函数\n",
    "# 省略了之前定义的 TextDataset 类和 collate_fn 函数...\n",
    "\n",
    "# 加载分词器和配置\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "config.lora_rank = 4\n",
    "config.lora_alpha = 32\n",
    "\n",
    "# 创建验证数据集和数据加载器\n",
    "valid_file_path = 'D:\\\\wet_files\\\\reduced_valid_part_0.txt'\n",
    "block_size = 128\n",
    "valid_dataset = TextDataset(tokenizer, valid_file_path, block_size)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 加载训练好的模型\n",
    "model_save_path = 'D:\\\\wet_files\\\\trained_model.pt'\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# 设置模型的设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_batches = len(valid_loader)\n",
    "for i, batch in enumerate(valid_loader, start=1):\n",
    "    inputs = {'input_ids': batch.to(device), 'labels': batch.to(device)}\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()\n",
    "    print(f\"Progress: {i}/{total_batches}, Current Batch Loss: {loss.item()}\", end='\\r')\n",
    "avg_loss = total_loss / total_batches\n",
    "print(f\"\\nValidation Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14085f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: 2120/2120 batches processed.\n",
      "Trained model has lower perplexity (34.23) compared to the original GPT-2 model (411.32), which is an improvement of 91.68%.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 定义 TextDataset 类和 collate_fn 函数\n",
    "# 省略了之前定义的 TextDataset 类和 collate_fn 函数...\n",
    "\n",
    "# 定义评估困惑度的函数\n",
    "def evaluate_perplexity(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = len(data_loader)\n",
    "    for i, batch in enumerate(data_loader, start=1):\n",
    "        inputs = {'input_ids': batch.to(device), 'labels': batch.to(device)}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Evaluating: {i}/{total_batches} batches processed.\", end='\\r')\n",
    "    avg_loss = total_loss / total_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "# 加载分词器和配置\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "config.lora_rank = 4\n",
    "config.lora_alpha = 32\n",
    "\n",
    "# 创建验证数据集和数据加载器\n",
    "valid_file_path = 'D:\\\\wet_files\\\\reduced_valid_part_0.txt'\n",
    "block_size = 128\n",
    "valid_dataset = TextDataset(tokenizer, valid_file_path, block_size)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 设置模型的设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载训练好的模型\n",
    "model_save_path = 'D:\\\\wet_files\\\\trained_model.pt'\n",
    "trained_model = GPT2LMHeadModel(config)\n",
    "trained_model.load_state_dict(torch.load(model_save_path))\n",
    "trained_model.to(device)\n",
    "\n",
    "# 加载原始的 GPT-2 模型\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "original_model.to(device)\n",
    "\n",
    "# 评估两个模型的困惑度\n",
    "trained_model_perplexity = evaluate_perplexity(trained_model, valid_loader, device)\n",
    "original_model_perplexity = evaluate_perplexity(original_model, valid_loader, device)\n",
    "\n",
    "# 比较两个模型的困惑度\n",
    "if trained_model_perplexity < original_model_perplexity:\n",
    "    improvement_percentage = (1 - trained_model_perplexity / original_model_perplexity) * 100\n",
    "    print(f\"\\nTrained model has lower perplexity ({trained_model_perplexity:.2f}) compared to the original GPT-2 model ({original_model_perplexity:.2f}), which is an improvement of {improvement_percentage:.2f}%.\")\n",
    "else:\n",
    "    improvement_percentage = (1 - original_model_perplexity / trained_model_perplexity) * 100\n",
    "    print(f\"\\nOriginal GPT-2 model has lower perplexity ({original_model_perplexity:.2f}) compared to the trained model ({trained_model_perplexity:.2f}), which is an improvement of {improvement_percentage:.2f}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e9a47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练前的 LM Head 参数的副本\n",
    "lm_head_params_before = {name: param.clone() for name, param in model.named_parameters() if 'lm_head' in name}\n",
    "\n",
    "\n",
    "\n",
    "# 获取训练后的 LM Head 参数\n",
    "lm_head_params_after = {name: param for name, param in model.named_parameters() if 'lm_head' in name}\n",
    "\n",
    "# 检查参数是否有变化\n",
    "for name, param_before in lm_head_params_before.items():\n",
    "    param_after = lm_head_params_after[name]\n",
    "    if not torch.equal(param_before, param_after):\n",
    "        print(f\"Parameter {name} has changed during training.\")\n",
    "    else:\n",
    "        print(f\"Parameter {name} has not changed during training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c807a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lm_head' in name:\n",
    "        print(f\"{name}: requires_grad = {param.requires_grad}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "301eb258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Perplexity: 34.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2 Model Perplexity: 411.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义 TextDataset 类和 collate_fn 函数\n",
    "# 省略了之前定义的 TextDataset 类和 collate_fn 函数...\n",
    "\n",
    "# 定义评估困惑度的函数\n",
    "def evaluate_perplexity(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with tqdm(data_loader, desc=\"Evaluating\", leave=False) as progress_bar:\n",
    "        for batch in progress_bar:\n",
    "            inputs = {'input_ids': batch.to(device), 'labels': batch.to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "# 加载分词器和配置\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "config.lora_rank = 4\n",
    "config.lora_alpha = 32\n",
    "\n",
    "# 创建验证数据集和数据加载器\n",
    "valid_file_path = 'D:\\\\wet_files\\\\reduced_valid_part_0.txt'\n",
    "block_size = 128\n",
    "valid_dataset = TextDataset(tokenizer, valid_file_path, block_size)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 设置模型的设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载训练好的模型\n",
    "model_save_path = 'D:\\\\wet_files\\\\trained_model.pt'\n",
    "trained_model = GPT2LMHeadModel(config)\n",
    "trained_model.load_state_dict(torch.load(model_save_path))\n",
    "trained_model.to(device)\n",
    "\n",
    "# 加载原始的 GPT-2 模型\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "original_model.to(device)\n",
    "\n",
    "# 评估两个模型的困惑度\n",
    "trained_model_perplexity = evaluate_perplexity(trained_model, valid_loader, device)\n",
    "print(f\"Trained Model Perplexity: {trained_model_perplexity:.2f}\")\n",
    "\n",
    "original_model_perplexity = evaluate_perplexity(original_model, valid_loader, device)\n",
    "print(f\"Original GPT-2 Model Perplexity: {original_model_perplexity:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71902a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
